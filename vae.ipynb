{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"checkpoints\") / f\"vae_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "dataset_dir = Path(\"dataset\")\n",
    "\n",
    "config = {\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-2,\n",
    "    'batch_size': 32,\n",
    "    'kl_weight': 0.1,\n",
    "    'aa_weight': 0.1,\n",
    "    'dim': 1,\n",
    "    'dt':\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data = True\n",
    "hamiltonians = []\n",
    "if generate_data:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=config['batch_size'])\n",
    "test_loader = torch.utils.data.DataLoader(train_dset, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE code modified from: https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py\n",
    "class Encoder:\n",
    "    def __init__(self, dim: int, layers: int):\n",
    "        self.dim = dim\n",
    "        self.num_layers = layers\n",
    "\n",
    "        modules = []\n",
    "        for _ in range(self.num_layers):\n",
    "            modules.append(nn.Linear(self.dim, self.dim))\n",
    "        \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "        self.fc_mu = nn.Linear(self.dim, self.dim)\n",
    "        self.fc_var = nn.Linear(self.dim, self.dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_var(x)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, dim: int, layers: int):\n",
    "        self.dim = dim\n",
    "        self.num_layers = layers\n",
    "\n",
    "        modules = []\n",
    "        for _ in range(self.num_layers):\n",
    "            modules.append(nn.Linear(self.dim, self.dim))\n",
    "        \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "class ActionAngleVAE:\n",
    "    def __init__(self, layers: int, dim: int, dt: float, n):\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # Dimension of the problem (eg 1 for SHO)\n",
    "        self.dim = dim\n",
    "        # Integration timestep\n",
    "        self.dt = dt\n",
    "        # Integration timesteps\n",
    "        self.n = n\n",
    "\n",
    "        self.encoder = Encoder(self.dim*2, self.num_layers)\n",
    "        self.decoder = Decoder(self.dim*2, self.num_layers)\n",
    "    \n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape [batch, n, dim*2]\n",
    "        mu, logvar = self.encoder(x)\n",
    "\n",
    "        J = z[:, :, :self.dim]\n",
    "        phi_dot = z[:, :, self.dim:]\n",
    "        phi = self.integrate_phi_dot(phi_dot)\n",
    "        \n",
    "        z = self.reparameterize(torch.stack(J, phi), logvar)\n",
    "\n",
    "        return self.decoder(z), mu, logvar, phi\n",
    "\n",
    "    def integrate_phi_dot(self, phi_dot: torch.Tensor) -> torch.Tensor:\n",
    "        phi = torch.zeros_like(phi_dot)\n",
    "        for i in range(1, self.n):\n",
    "            phi[:,i] + phi_dot[:,i] * self.dt\n",
    "        \n",
    "        return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon, mu, logvar, phi, x, config):\n",
    "    recon_loss = F.mse_loss(recon, x)\n",
    "\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "\n",
    "    # Minimize the change in action and derivative of angle over time\n",
    "    J = mu[:, :, :config['dim']]\n",
    "    phi_dot = mu[:, :, config['dim']:]\n",
    "    aa_loss = torch.sum(J - J.mean(), axis=1) + torch.sum(phi_dot - phi_dot.mean(), axis=1)\n",
    "\n",
    "    loss = recon_loss + config['kl_weight']*kl_loss + config['aa_weight']*aa_loss\n",
    "\n",
    "    return loss, recon_loss, kl_loss, aa_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActionAngleVAE().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pbar = \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(config[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m      3\u001b[39m     total_loss, total_mse, total_kl, total_aa = \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "running_mse = []\n",
    "running_kl = []\n",
    "running_aa = []\n",
    "\n",
    "pbar = tqdm(range(config['epochs']))\n",
    "for epoch in pbar:\n",
    "    total_loss, total_mse, total_kl, total_aa = 0.0, 0.0, 0.0, 0.0\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon, mu, logvar, phi, J = model(x)\n",
    "        loss, mse_loss, kl_loss, aa_loss = loss_fn(recon, mu, logvar, phi)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_mse += mse_loss\n",
    "        total_kl += kl_loss\n",
    "        total_aa += aa_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    pbar.set_postfix_str(f\"Loss: {total_loss/len(train_loader):.2f}\")\n",
    "    running_loss.append(total_loss / len(train_loader))\n",
    "    running_mse.append(total_mse / len(train_loader))\n",
    "    running_kl.append(total_kl / len(train_loader))\n",
    "    running_aa.append(total_aa / len(train_loader))\n",
    "\n",
    "torch.save(model.state_dict(), checkpoint_dir / 'ckpt.pt')\n",
    "with open(checkpoint_dir/'config.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss, label='Total Loss')\n",
    "plt.plot(running_mse, label='MSE Loss')\n",
    "plt.plot(running_kl, label='KL Loss')\n",
    "plt.plot(running_aa, label='AA Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_plot(axs, J, phi, **kwargs):\n",
    "    axs[0].plot(torch.cos(phi)*J, torch.sin(phi)*J, **kwargs)\n",
    "    axs[1].plot(phi, J, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(test_loader))\n",
    "i = 0\n",
    "orig_J = sample_batch[i, :config['dim']]\n",
    "orig_phi = sample_batch[i, config['dim']:]\n",
    "\n",
    "true_J, true_phi = AA_transformation(orig_J, orig_phi)\n",
    "\n",
    "mu, logvar = model.encoder(sample_batch)\n",
    "J = mu[:config['dim']]\n",
    "phi_dot = mu[config['dim']:]\n",
    "phi = model.integrate_phi_dot(phi_dot)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "aa_plot(axs, orig_J, orig_phi, {'label': 'original', 'c': 'grey'})\n",
    "aa_plot(axs, true_J, true_phi, {'label': 'original', 'c': 'black'})\n",
    "aa_plot(axs, J[i], phi[i], {'label': 'predicted'})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
